{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scrapy Notes\n",
    "\n",
    "print this inside scrapy shell\n",
    "\n",
    "scrapy shell http://quotes.toscrape.com/random\n",
    "\n",
    "### response.text \n",
    "will give you web page source code\n",
    "\n",
    "### below result has given on the basis of upper data website source code\n",
    "\n",
    "In [4]: response.css('small.author')\n",
    "Out[4]: [<Selector xpath=\"descendant-or-self::small[@class and contains(concat(' ', normalize-space(@class), ' '), ' author ')]\" data='<small class=\"author\" itemprop=\"autho...'>]\n",
    "    \n",
    "    \n",
    "    \n",
    "In [5]: response.css('small.author').extract()\n",
    "Out[5]: ['<small class=\"author\" itemprop=\"author\">Charles Bukowski</small>']\n",
    "\n",
    "In [6]: response.css('small.author::text').extract()[0]\n",
    "Out[6]: 'Charles Bukowski'\n",
    "\n",
    "In [7]: response.css('span.text').extract()\n",
    "Out[7]: ['<span class=\"text\" itemprop=\"text\">“That\\'s the problem with drinking, I thought, as I poured myself a drink. If something bad happens you drink in an attempt to forget; if something good happens you drink in order to celebrate; and if nothing happens you drink to make something happen.”</span>']\n",
    "\n",
    "In [8]: response.css('a.tag::text').extract()\n",
    "Out[8]: ['alcohol']\n",
    "\n",
    "In [9]: response.css('a.tag')\n",
    "Out[9]: [<Selector xpath=\"descendant-or-self::a[@class and contains(concat(' ', normalize-space(@class), ' '), ' tag ')]\" data='<a class=\"tag\" href=\"/tag/alcohol/pag...'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your First Scrapy Spider"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### scrapy genspider quotes toscrape.com \n",
    "command to generate spider will be created in same dir\n",
    "\n",
    "\n",
    "import scrapy\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "    \n",
    "### here inside parse function we can create dictionary and also create a file by following command\n",
    "scrapy runspider quotes.py -o file.json\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping from multiple items"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#while scraping we can use extract_first()\n",
    "\n",
    "#suppose we have multiplr data from same type div element than first we will select that element like below\n",
    "\n",
    "quote = responce.css('div.quote')\n",
    "\n",
    "# here again we can further extract the data from quote\n",
    "\n",
    "quote.css('small.tags::text').extract() / extract_first()\n",
    "\n",
    "# also same thing can be put into json file. via spider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  scrap the data from paginations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#follow pagination\n",
    "next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "next_page_url = response.urljoin(next_oage_url)\n",
    "yield scrapy.Request(url = next_page_url , callback = self.parse)\n",
    "\n",
    "## this methode need to be used inside the parse function see actual code in repective folder\n",
    "\n",
    "#### we should also put verification so that we end the loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping from request under request\n",
    "\n",
    "### see code requat_under_request.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we are doing now post request , keeping the code into file name \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url for better methode docs - https://docs.scrapy.org/en/latest/topics/request-response.html?highlight=formrequest#formrequest-objects\n",
    "\n",
    "see the code for clarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pulling js content inside the browser "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for that first we need to call docker by\n",
    "\n",
    "docker pull scrapinghub/splash\n",
    "\n",
    "# You need to make sure that the previous container you launched is killed, before launching a new one that uses the same port.\n",
    "\n",
    "# docker container ls\n",
    "# docker rm -f <container-name>\n",
    "\n",
    "#docker run -p 8050:8050 scrapinghub/splash\n",
    "\n",
    "later we can run the code from spider\n",
    "\n",
    "Since we are using the Docker Toolbox, we need to use our docker-machine ip. Go to your terminal and type:\n",
    "\n",
    "docker-machine ip\n",
    "Now that you have the right ip, go to your settings.py and set\n",
    "\n",
    "SPLASH_URL = http://[docker-machine ip]:8050\n",
    "\n",
    "Confuguration is available for docker here\n",
    "https://splash.readthedocs.io/en/stable/install.html#os-x-docker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running crawler in cloud"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Code for starting a project\n",
    "\n",
    "scrapy startproject name\n",
    "\n",
    "#later inside spider project we can copy as many as we want\n",
    "\n",
    "scrapy list\n",
    "it will give you lisst of spider\n",
    "\n",
    "#now we can run like\n",
    "scrapy crawl name\n",
    "\n",
    "for hosting the crawler log in to https://app.scrapinghub.com/p/456506/deploy?state=deploy\n",
    "and create the project there\n",
    "\n",
    "later install shub  by pip install shub\n",
    "\n",
    "shub login\n",
    "## that will be asking for API Keys\n",
    "\n",
    "once login than\n",
    "\n",
    "shub deploy ----- and enter project ID that we will get in url it self\n",
    "\n",
    "then from the panel we can run and enjoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
